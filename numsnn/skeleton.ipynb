{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cca22115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# MNIST DIGIT CLASSIFIER (PyTorch)\n",
    "# -----------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import gradio as gr\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df2a9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. LOAD DATA\n",
    "# Transforms are preprocessing steps that get applied automatically to every image\n",
    "# you load from a dataset. \n",
    "# Think of transforms as a recipe that says:\n",
    "\n",
    "# “Every time you give me an image, do X, then Y, then Z to it.”\n",
    "# “For every MNIST image: convert it to a PyTorch tensor.\n",
    "# MNIST images come in as PIL images (Python Imaging Library).\n",
    "\n",
    "# But your neural network expects tensors.\n",
    "# -----------------------------\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a123ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transform_train,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "de572723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transform_test,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87def014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Make DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "unique_labels = sorted(set(train_dataset.targets.tolist()))\n",
    "print('Unique labels in training dataset:', unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "091eb8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 2. DEFINE NEURAL NETWORK\n",
    "# Neural Network with 1 hidden layer of 128 neurons\n",
    "# -----------------------------\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128);\n",
    "        self.fc2 = nn.Linear(128, 10);\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten image: (batch, 1, 28, 28) → (batch, 784)\n",
    "        x = x.view(-1, 28*28);\n",
    "        x = torch.relu(self.fc1(x));\n",
    "        x = self.fc2(x);\n",
    "        return x;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "138089f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu');\n",
    "model = SimpleNN().to(device);\n",
    "print(f'Using device: {device}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d12046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. LOSS FUNCTION + OPTIMIZER\n",
    "# -----------------------------\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d32ff29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Avg Loss: 0.3296\n",
      "Epoch 2/50, Avg Loss: 0.2715\n",
      "Epoch 2/50, Avg Loss: 0.2715\n",
      "Epoch 3/50, Avg Loss: 0.2330\n",
      "Epoch 3/50, Avg Loss: 0.2330\n",
      "Epoch 4/50, Avg Loss: 0.2281\n",
      "Epoch 4/50, Avg Loss: 0.2281\n",
      "Epoch 5/50, Avg Loss: 0.2342\n",
      "Epoch 5/50, Avg Loss: 0.2342\n",
      "Epoch 6/50, Avg Loss: 0.2211\n",
      "Epoch 6/50, Avg Loss: 0.2211\n",
      "Epoch 7/50, Avg Loss: 0.2084\n",
      "Epoch 7/50, Avg Loss: 0.2084\n",
      "Epoch 8/50, Avg Loss: 0.2109\n",
      "Epoch 8/50, Avg Loss: 0.2109\n",
      "Epoch 9/50, Avg Loss: 0.2073\n",
      "Epoch 9/50, Avg Loss: 0.2073\n",
      "Epoch 10/50, Avg Loss: 0.2084\n",
      "Epoch 10/50, Avg Loss: 0.2084\n",
      "Epoch 11/50, Avg Loss: 0.2114\n",
      "Epoch 11/50, Avg Loss: 0.2114\n",
      "Epoch 12/50, Avg Loss: 0.2122\n",
      "Epoch 12/50, Avg Loss: 0.2122\n",
      "Epoch 13/50, Avg Loss: 0.1970\n",
      "Epoch 13/50, Avg Loss: 0.1970\n",
      "Epoch 14/50, Avg Loss: 0.2014\n",
      "Epoch 14/50, Avg Loss: 0.2014\n",
      "Epoch 15/50, Avg Loss: 0.1985\n",
      "Epoch 15/50, Avg Loss: 0.1985\n",
      "Epoch 16/50, Avg Loss: 0.2066\n",
      "Epoch 16/50, Avg Loss: 0.2066\n",
      "Epoch 17/50, Avg Loss: 0.1915\n",
      "Epoch 17/50, Avg Loss: 0.1915\n",
      "Epoch 18/50, Avg Loss: 0.2001\n",
      "Epoch 18/50, Avg Loss: 0.2001\n",
      "Epoch 19/50, Avg Loss: 0.1979\n",
      "Epoch 19/50, Avg Loss: 0.1979\n",
      "Epoch 20/50, Avg Loss: 0.1846\n",
      "Epoch 20/50, Avg Loss: 0.1846\n",
      "Epoch 21/50, Avg Loss: 0.1934\n",
      "Epoch 21/50, Avg Loss: 0.1934\n",
      "Epoch 22/50, Avg Loss: 0.1908\n",
      "Epoch 22/50, Avg Loss: 0.1908\n",
      "Epoch 23/50, Avg Loss: 0.1966\n",
      "Epoch 23/50, Avg Loss: 0.1966\n",
      "Epoch 24/50, Avg Loss: 0.1946\n",
      "Epoch 24/50, Avg Loss: 0.1946\n",
      "Epoch 25/50, Avg Loss: 0.1906\n",
      "Epoch 25/50, Avg Loss: 0.1906\n",
      "Epoch 26/50, Avg Loss: 0.1897\n",
      "Epoch 26/50, Avg Loss: 0.1897\n",
      "Epoch 27/50, Avg Loss: 0.1863\n",
      "Epoch 27/50, Avg Loss: 0.1863\n",
      "Epoch 28/50, Avg Loss: 0.1949\n",
      "Epoch 28/50, Avg Loss: 0.1949\n",
      "Epoch 29/50, Avg Loss: 0.1956\n",
      "Epoch 29/50, Avg Loss: 0.1956\n",
      "Epoch 30/50, Avg Loss: 0.1999\n",
      "Epoch 30/50, Avg Loss: 0.1999\n",
      "Epoch 31/50, Avg Loss: 0.1946\n",
      "Epoch 31/50, Avg Loss: 0.1946\n",
      "Epoch 32/50, Avg Loss: 0.1945\n",
      "Epoch 32/50, Avg Loss: 0.1945\n",
      "Epoch 33/50, Avg Loss: 0.1850\n",
      "Epoch 33/50, Avg Loss: 0.1850\n",
      "Epoch 34/50, Avg Loss: 0.1962\n",
      "Epoch 34/50, Avg Loss: 0.1962\n",
      "Epoch 35/50, Avg Loss: 0.1877\n",
      "Epoch 35/50, Avg Loss: 0.1877\n",
      "Epoch 36/50, Avg Loss: 0.1921\n",
      "Epoch 36/50, Avg Loss: 0.1921\n",
      "Epoch 37/50, Avg Loss: 0.1889\n",
      "Epoch 37/50, Avg Loss: 0.1889\n",
      "Epoch 38/50, Avg Loss: 0.1914\n",
      "Epoch 38/50, Avg Loss: 0.1914\n",
      "Epoch 39/50, Avg Loss: 0.1974\n",
      "Epoch 39/50, Avg Loss: 0.1974\n",
      "Epoch 40/50, Avg Loss: 0.1849\n",
      "Epoch 40/50, Avg Loss: 0.1849\n",
      "Epoch 41/50, Avg Loss: 0.1891\n",
      "Epoch 41/50, Avg Loss: 0.1891\n",
      "Epoch 42/50, Avg Loss: 0.1859\n",
      "Epoch 42/50, Avg Loss: 0.1859\n",
      "Epoch 43/50, Avg Loss: 0.1937\n",
      "Epoch 43/50, Avg Loss: 0.1937\n",
      "Epoch 44/50, Avg Loss: 0.1827\n",
      "Epoch 44/50, Avg Loss: 0.1827\n",
      "Epoch 45/50, Avg Loss: 0.2006\n",
      "Epoch 45/50, Avg Loss: 0.2006\n",
      "Epoch 46/50, Avg Loss: 0.1871\n",
      "Epoch 46/50, Avg Loss: 0.1871\n",
      "Epoch 47/50, Avg Loss: 0.1800\n",
      "Epoch 47/50, Avg Loss: 0.1800\n",
      "Epoch 48/50, Avg Loss: 0.1832\n",
      "Epoch 48/50, Avg Loss: 0.1832\n",
      "Epoch 49/50, Avg Loss: 0.1829\n",
      "Epoch 49/50, Avg Loss: 0.1829\n",
      "Epoch 50/50, Avg Loss: 0.1844\n",
      "Epoch 50/50, Avg Loss: 0.1844\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. TRAINING LOOP\n",
    "# -----------------------------\n",
    "\n",
    "epochs = 50;\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device);\n",
    "        labels = labels.to(device);\n",
    "        \n",
    "        optimizer.zero_grad();\n",
    "\n",
    "        outputs = model(images);\n",
    "\n",
    "        loss = criterion(outputs, labels);\n",
    "\n",
    "        loss.backward(); #backprop\n",
    "\n",
    "        optimizer.step(); #update the weights based on the gradients\n",
    "\n",
    "        total_loss+=loss.item()*images.size(0); #this is to get the total loss for the epoch\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {total_loss/len(train_loader.dataset):.4f}\") #print above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c6e829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 97.95%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. EVALUATION\n",
    "# -----------------------------\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b038ae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. TEST SINGLE PREDICTION\n",
    "# -----------------------------\n",
    "# -----------------------------\n",
    "# 6. TEST SINGLE PREDICTION\n",
    "# -----------------------------\n",
    "# ------------------------------\n",
    "# Gradio Sketchpad gives you:\n",
    "\n",
    "# * a full-color NumPy array\n",
    "\n",
    "# * black digit on white background\n",
    "\n",
    "# * large resolution\n",
    "\n",
    "# * no consistent scale\n",
    "#\n",
    "# Hence the preprocessing\n",
    "# ------------------------------\n",
    "\n",
    "def preprocess_image(image):\n",
    "    sketch_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),                      # NumPy → PIL\n",
    "    transforms.Grayscale(),                       # ensure 1 channel\n",
    "    transforms.Resize((28, 28)),                  # 28x28 like MNIST\n",
    "    transforms.Lambda(lambda img: ImageOps.invert(img)),  # invert colors\n",
    "    transforms.ToTensor(),                        # → tensor, shape (1,28,28), values in [0,1]\n",
    "    ])\n",
    "    # Gradio Sketchpad sometimes passes a dict with 'composite'\n",
    "    if isinstance(image, dict):\n",
    "        image = image['composite']   # this is a NumPy array\n",
    "    \n",
    "    # Apply the preprocessing transform\n",
    "    img_tensor = sketch_transform(image)  # (1, 28, 28)\n",
    "    \n",
    "    # Add batch dimension → (1, 1, 28, 28)\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "    return img_tensor\n",
    "\n",
    "def predict_digit(image):\n",
    "    # --- STEP 1: CHECK IF SOMETHING HAS BEEN DRAWN ---\n",
    "    if image is None: return \"Draw something!\"\n",
    "\n",
    "    # --- STEP 2: PREPROCESS THE IMAGE ---\n",
    "    img_tensor = preprocess_image(image)\n",
    "    \n",
    "    # --- STEP 3: RUN THE MODEL ---\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)\n",
    "        \n",
    "        # Get the index of the highest score (the predicted digit)\n",
    "        predicted_digit = torch.argmax(prediction).item()\n",
    "        \n",
    "    return str(predicted_digit)\n",
    "\n",
    "# UI Setup\n",
    "interface = gr.Interface(fn=predict_digit, inputs=gr.Sketchpad(label=\"Draw Here\"), outputs=\"label\")\n",
    "interface.queue().launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
