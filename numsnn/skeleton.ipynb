{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cca22115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aa/Desktop/coding/p3ai/numsnn/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# MNIST DIGIT CLASSIFIER (PyTorch)\n",
    "# -----------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import gradio as gr\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu');\n",
    "print(f'Using device: {device}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df2a9957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. LOAD DATA\n",
    "# Transforms are preprocessing steps that get applied automatically to every image\n",
    "# you load from a dataset. \n",
    "# Think of transforms as a recipe that says:\n",
    "\n",
    "# “Every time you give me an image, do X, then Y, then Z to it.”\n",
    "# “For every MNIST image: convert it to a PyTorch tensor.\n",
    "# MNIST images come in as PIL images (Python Imaging Library).\n",
    "\n",
    "# But your neural network expects tensors.\n",
    "# -----------------------------\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a123ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=True,\n",
    "    transform=transform_train,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de572723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test dataset\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\",\n",
    "    train=False,\n",
    "    transform=transform_test,\n",
    "    download=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87def014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in training dataset: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# Make DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "unique_labels = sorted(set(train_dataset.targets.tolist()))\n",
    "print('Unique labels in training dataset:', unique_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091eb8d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 2. DEFINE NEURAL NETWORK\n",
    "# -----------------------------\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.norm = nn.LayerNorm(128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = torch.relu(self.norm(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "model = SimpleNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d13bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleCNN(\n",
      "  (conv_layers): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=12544, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 2. DEFINE CNN MODEL (better/alternate method that uses a convolutional neural network)\n",
    "# The reason that we would want to use a CNN over a NN is that CNNs are just better at image related tasks, so we should expect better performance from this model.\n",
    "# -----------------------------\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_layers = nn.Sequential(nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(), nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(2, 2),)\n",
    "        self.fc = nn.Sequential(nn.Linear(64 * 14 * 14, 128), nn.ReLU(), nn.Linear(128, 10))\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "model = SimpleCNN().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47f85ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MNISTNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(32, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(4, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(64, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(8, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (9): Dropout(p=0.2, inplace=False)\n",
       "    (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): ReLU()\n",
       "    (13): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (14): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (16): ReLU()\n",
       "    (17): SEBlock(\n",
       "      (se): Sequential(\n",
       "        (0): AdaptiveAvgPool2d(output_size=1)\n",
       "        (1): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (4): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=6272, out_features=512, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#squeeze excitement block --> attention mechanism that helps to improve performance by focusing on the most important features from the convolutional layers\n",
    "#this is especially useful in image classification tasks where certain features are more relevant for distinguishing between classes like numbers in MNIST\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, c, r=8):\n",
    "        super().__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1), #global avg pooling to create channel descriptors to capture global spatial information\n",
    "            nn.Conv2d(c, c // r, 1), #reduce channel dimensionality for higher computational efficiency\n",
    "            nn.ReLU(inplace=True), #activation function (relu)\n",
    "            nn.Conv2d(c // r, c, 1), #restore channel dimensionality to original size\n",
    "            nn.Sigmoid(), #squash values to 0 and 1 to represent importance weights\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.se(x) #get importance weights of each channel\n",
    "        return x * w #apply the weights to the input feature map to emphasize important features and suppress less important ones;\n",
    "\n",
    "\n",
    "class MNISTNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, padding=1), #conv layer 1\n",
    "            nn.BatchNorm2d(32), #norm layer to stabilize and speed up training a little\n",
    "            nn.ReLU(), #activation func\n",
    "            SEBlock(32), #sqz excite block of 32 for focus\n",
    "            nn.Conv2d(32, 64, 3, padding=1), #conv layer 2\n",
    "            nn.BatchNorm2d(64), #norm layer\n",
    "            nn.ReLU(), #activation func\n",
    "            SEBlock(64), #sqz excite block with 64 chann\n",
    "            nn.MaxPool2d(2), #downsample feature maps to reduce spatial dimensions and computation load improving efficiency\n",
    "            nn.Dropout(0.2), #dropout to prevent some overfitting (might need tuning later)\n",
    "            nn.Conv2d(64, 128, 3, padding=1), #conv layer 3\n",
    "            nn.BatchNorm2d(128), #norm layer\n",
    "            nn.ReLU(), #activation func\n",
    "            SEBlock(128), #sqz excite block with 128 chann\n",
    "            nn.Conv2d(128, 128, 3, padding=1), #conv layer 4\n",
    "            nn.BatchNorm2d(128), #norm layer (its 128 bc output of lsat conv was 128 chann)\n",
    "            nn.ReLU(), #activation func\n",
    "            SEBlock(128), #sqz excite block with 128 chann\n",
    "            nn.MaxPool2d(2), #downsample feature maps again for efficiency\n",
    "            nn.Dropout(0.3), #dropout to stop more overfitting\n",
    "        )\n",
    "        self.classifier = nn.Sequential( #this whole chunk is just to classify based on the features taken from conv layers\n",
    "            nn.Flatten(), #flatten the 2d feat map into a 1d vec\n",
    "            nn.Linear(128 * 7 * 7, 512), #fully conn layer is 128*7*7 bc last maxpool made the 28x28 img to 7x7\n",
    "            nn.ReLU(), #activ func \n",
    "            nn.Dropout(0.5), #drop to not overfit\n",
    "            nn.Linear(512, 10), #final out layer for 0-9 digits;\n",
    "        )\n",
    "        self.apply(self._init) #init ws\n",
    "\n",
    "    def _init(self, m): #w init func\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)): #if conv or lin layer\n",
    "            nn.init.kaiming_normal_(m.weight) #he init for relu\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias) #bias init to 0 if exists\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.features(x)) #pass through the feature extractor then the classifier on model eval\n",
    "    \n",
    "model = MNISTNet() #model inst\n",
    "model.load_state_dict(torch.load(\"99.68%.pth\", map_location=\"cpu\")) #call load model using above architecture and saved .pth file on local\n",
    "model.eval() #set to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95d12046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 3. LOSS FUNCTION + OPTIMIZER\n",
    "# -----------------------------\n",
    "criterion = nn.CrossEntropyLoss();\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d32ff29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5w/l2b9qlms07g2ky4_nwq8kxjh0000gn/T/ipykernel_31892/1209600713.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
      "Epoch 1/35:   0%|          | 0/938 [00:00<?, ?it/s]/var/folders/5w/l2b9qlms07g2ky4_nwq8kxjh0000gn/T/ipykernel_31892/1209600713.py:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
      "Epoch 1/35: 100%|██████████| 938/938 [01:24<00:00, 11.09it/s, loss=0.0127] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35, Avg Loss: 0.2162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/35: 100%|██████████| 938/938 [01:17<00:00, 12.07it/s, loss=0.0628] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/35, Avg Loss: 0.0580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/35: 100%|██████████| 938/938 [01:16<00:00, 12.21it/s, loss=0.00112] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/35, Avg Loss: 0.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/35: 100%|██████████| 938/938 [-148:57:01<00:00, -0.00it/s, loss=0.00508] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/35, Avg Loss: 0.0336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/35: 100%|██████████| 938/938 [01:53<00:00,  8.27it/s, loss=0.0021]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/35, Avg Loss: 0.0280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/35: 100%|██████████| 938/938 [01:49<00:00,  8.57it/s, loss=0.00147] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/35, Avg Loss: 0.0233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/35: 100%|██████████| 938/938 [01:55<00:00,  8.10it/s, loss=0.0516]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/35, Avg Loss: 0.0226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/35: 100%|██████████| 938/938 [01:53<00:00,  8.23it/s, loss=0.0214]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/35, Avg Loss: 0.0184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/35: 100%|██████████| 938/938 [01:51<00:00,  8.39it/s, loss=0.0578]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/35, Avg Loss: 0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/35: 100%|██████████| 938/938 [01:57<00:00,  7.98it/s, loss=0.00303] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/35, Avg Loss: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/35: 100%|██████████| 938/938 [01:51<00:00,  8.39it/s, loss=0.00141] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/35, Avg Loss: 0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/35: 100%|██████████| 938/938 [01:56<00:00,  8.04it/s, loss=0.000108]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/35, Avg Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/35: 100%|██████████| 938/938 [01:58<00:00,  7.94it/s, loss=0.000179]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/35, Avg Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/35: 100%|██████████| 938/938 [01:59<00:00,  7.88it/s, loss=4.02e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/35, Avg Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/35: 100%|██████████| 938/938 [01:52<00:00,  8.33it/s, loss=1.69e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/35, Avg Loss: 0.0104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/35: 100%|██████████| 938/938 [01:54<00:00,  8.20it/s, loss=0.00078] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/35, Avg Loss: 0.0090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/35: 100%|██████████| 938/938 [01:59<00:00,  7.86it/s, loss=0.00832] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/35, Avg Loss: 0.0099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/35: 100%|██████████| 938/938 [01:43<00:00,  9.07it/s, loss=0.000463]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/35, Avg Loss: 0.0089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/35: 100%|██████████| 938/938 [02:03<00:00,  7.58it/s, loss=8.49e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/35, Avg Loss: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/35: 100%|██████████| 938/938 [01:54<00:00,  8.17it/s, loss=8.53e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/35, Avg Loss: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/35: 100%|██████████| 938/938 [02:10<00:00,  7.16it/s, loss=0.00407] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/35, Avg Loss: 0.0086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/35: 100%|██████████| 938/938 [01:57<00:00,  7.98it/s, loss=0.000281]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/35, Avg Loss: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/35: 100%|██████████| 938/938 [01:42<00:00,  9.13it/s, loss=0.00289] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/35, Avg Loss: 0.0075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/35: 100%|██████████| 938/938 [02:09<00:00,  7.23it/s, loss=1.23e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/35, Avg Loss: 0.0060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/35: 100%|██████████| 938/938 [02:58<00:00,  5.26it/s, loss=4.56e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/35, Avg Loss: 0.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/35: 100%|██████████| 938/938 [03:13<00:00,  4.86it/s, loss=0.000193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/35, Avg Loss: 0.0070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/35: 100%|██████████| 938/938 [03:19<00:00,  4.69it/s, loss=8.08e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/35, Avg Loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/35: 100%|██████████| 938/938 [03:03<00:00,  5.11it/s, loss=1.53e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/35, Avg Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/35: 100%|██████████| 938/938 [1:40:36<00:00,  6.43s/it, loss=0.000132]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/35, Avg Loss: 0.0052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/35: 100%|██████████| 938/938 [01:40<00:00,  9.32it/s, loss=7.32e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/35, Avg Loss: 0.0062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/35: 100%|██████████| 938/938 [02:25<00:00,  6.43it/s, loss=3.51e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/35, Avg Loss: 0.0051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/35: 100%|██████████| 938/938 [01:44<00:00,  9.00it/s, loss=3.26e-6] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/35, Avg Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/35: 100%|██████████| 938/938 [02:41<00:00,  5.80it/s, loss=2.16e-7] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/35, Avg Loss: 0.0050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/35: 100%|██████████| 938/938 [02:57<00:00,  5.28it/s, loss=2.14e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/35, Avg Loss: 0.0053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/35: 100%|██████████| 938/938 [02:38<00:00,  5.91it/s, loss=0.0297]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/35, Avg Loss: 0.0063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 4. TRAINING LOOP\n",
    "# -----------------------------\n",
    "\n",
    "epochs = 35\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "    for images, labels in pbar:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        labels = labels.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * images.size(0)\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59c6e829",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m correct = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[43mmodel\u001b[49m.eval()\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m test_loader:\n",
      "\u001b[31mNameError\u001b[39m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 5. EVALUATION\n",
    "# -----------------------------\n",
    "correct = 0\n",
    "total = 0\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total+=labels.size(0)\n",
    "        correct+=(predicted==labels).sum().item()\n",
    "\n",
    "print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b038ae14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# 6. TEST SINGLE PREDICTION\n",
    "# -----------------------------\n",
    "# Gradio sketchpad returns a full-color NumPy array (H,W,3).\n",
    "# MNIST images are grayscale (1x28x28) and normalized.\n",
    "# This preprocessing converts user drawings into MNIST format.\n",
    "# -----------------------------\n",
    "\n",
    "# MNIST normalization values:\n",
    "MNIST_MEAN = (0.1307,)\n",
    "MNIST_STD  = (0.3081,)\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Convert Gradio Sketchpad output to a normalized 1x28x28 tensor.\"\"\"\n",
    "\n",
    "    # Gradio may pass {'composite': array}\n",
    "    if isinstance(image, dict) and \"composite\" in image:\n",
    "        image = image[\"composite\"]\n",
    "\n",
    "    # Define preprocessing pipeline\n",
    "    sketch_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),                      # NumPy → PIL\n",
    "        transforms.Grayscale(num_output_channels=1),  # Convert to 1 channel\n",
    "        transforms.Resize((28, 28)),                  # Match MNIST input\n",
    "        transforms.Lambda(lambda img: ImageOps.invert(img)),  \n",
    "        transforms.ToTensor(),                        # → (1, 28, 28), values in [0,1]\n",
    "        transforms.Normalize(MNIST_MEAN, MNIST_STD),  # Match MNIST training normalization\n",
    "    ])\n",
    "\n",
    "    tensor = sketch_transform(image)                  # Shape: (1, 28, 28)\n",
    "    tensor = tensor.unsqueeze(0)                      # Shape: (1, 1, 28, 28)\n",
    "    return tensor.to(device)                          # Move to same device as model\n",
    "\n",
    "\n",
    "def predict_digit(image):\n",
    "    \"\"\"Take raw Sketchpad input → return predicted digit + confidence.\"\"\"\n",
    "    \n",
    "    if image is None:\n",
    "        return \"Draw something!\"\n",
    "\n",
    "    # Convert to model input format\n",
    "    input_tensor = preprocess_image(image)\n",
    "\n",
    "    # Ensure model is in eval mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_tensor)\n",
    "        \n",
    "        # turn logits into probabilities\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "\n",
    "        # predicted class index\n",
    "        predicted_class = torch.argmax(probs, dim=1).item()\n",
    "\n",
    "        # confidence of that class\n",
    "        confidence = probs[0, predicted_class].item()\n",
    "\n",
    "    # return nicely formatted output\n",
    "    return f\"{predicted_class}  ({confidence * 100:.2f}% confidence)\"\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# GRADIO UI\n",
    "# -----------------------------\n",
    "interface = gr.Interface(\n",
    "    fn=predict_digit,\n",
    "    inputs=gr.Sketchpad(label=\"Draw Here\"),\n",
    "    outputs=\"label\",\n",
    "    live=False,\n",
    ")\n",
    "\n",
    "interface.queue().launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.11.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
