{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c1360b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aa/Desktop/coding/p3ai/t2final/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 76/76 [00:00<00:00, 1353.71it/s, Materializing param=transformer.wte.weight]            \n",
      "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "print(\"Loading AI model...\")\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9cda4bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prompt: 'Upon the lip of the singularity, where light screams into silence and gravity drinks the dust of dead stars,'\n",
      "Generating...\n",
      "\n",
      "--- Poem Start ---\n",
      "Upon the lip of the singularity, where light screams into silence and gravity drinks the dust of dead stars,\n",
      "Line 1: it is evident that this noise has been echoed many times since—a manifestation not\n",
      "Line 2: only on Earth itself, but in its occupants as well.\n",
      "Line 3: True to music; however bitter they might be for anyone contemplating a green lost\n",
      "Line 4: cause: an abundant selection with depressingly unpleasant lyrics starring Audre Lorde adorning Lazer drum\n",
      "\n",
      "--- Final Composition ---\n",
      "Upon the lip of the singularity, where light screams into silence and gravity drinks the dust of dead stars,\n",
      "it is evident that this noise has been echoed many times since—a manifestation not\n",
      "only on Earth itself, but in its occupants as well.\n",
      "True to music; however bitter they might be for anyone contemplating a green lost\n",
      "cause: an abundant selection with depressingly unpleasant lyrics starring Audre Lorde adorning Lazer drum\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. The Prompt: Primed for Abstract Cosmic Horror/Beauty\n",
    "# We end with a comma to encourage continuation rather than a full stop.\n",
    "txt = \"Upon the lip of the singularity, where light screams into silence and gravity drinks the dust of dead stars,\"\n",
    "\n",
    "print(f\"Start prompt: '{txt}'\")\n",
    "print(\"Generating...\\n\")\n",
    "\n",
    "# 2. Tuning for \"Beautiful/Abstract\" Generation\n",
    "# Temp 1.0 + Top-P 0.92 is the \"Goldilocks\" zone for poetry.\n",
    "# It allows for rare words (high temp) but cuts off nonsense (top-p).\n",
    "temp = 1.0        # Allows for creative/rare word choices\n",
    "top_p = 0.92      # \"Nucleus\" threshold: cuts off the tail of garbage tokens\n",
    "rep_penalty = 1.2 # Stronger penalty prevents \"the the of the\" loops\n",
    "nlns = 4          # Number of lines\n",
    "wpln = 5          # Min words per line\n",
    "wplnmax = 14      # Max words per line\n",
    "\n",
    "# 3. Setup\n",
    "lns = []\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Initial encoding\n",
    "input_ids = tokenizer.encode(txt, return_tensors=\"pt\").to(device)\n",
    "current_ids = input_ids\n",
    "\n",
    "# KV-Caching Variable (The Speed Optimization)\n",
    "# This stores the \"memory\" of previous tokens so we don't re-process the whole poem every loop.\n",
    "past_key_values = None \n",
    "\n",
    "# Pre-calculate forbidden tokens (Newlines/EOS) to stop early breaking\n",
    "newline_tokens = tokenizer.encode(\"\\n\", add_special_tokens=False)\n",
    "eos_token = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"--- Poem Start ---\\n{txt}\")\n",
    "\n",
    "for i in range(nlns):\n",
    "    lntxt_ids = [] # Store token IDs for the current line\n",
    "    comp = False\n",
    "    \n",
    "    while not comp:\n",
    "        with torch.no_grad():\n",
    "            # OPTIMIZATION: Only feed the *last* token if we have cache\n",
    "            if past_key_values is None:\n",
    "                outputs = model(current_ids, use_cache=True)\n",
    "            else:\n",
    "                outputs = model(current_ids[:, -1:], past_key_values=past_key_values, use_cache=True)\n",
    "            \n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            past_key_values = outputs.past_key_values # Update cache\n",
    "\n",
    "        # A. Apply Repetition Penalty (prevent looping)\n",
    "        # We penalize all tokens currently in the context\n",
    "        for t_id in set(input_ids[0].tolist() + lntxt_ids):\n",
    "            # If logit is positive, divide; if negative, multiply (to make it smaller/more negative)\n",
    "            if logits[0, t_id] < 0:\n",
    "                logits[0, t_id] *= rep_penalty\n",
    "            else:\n",
    "                logits[0, t_id] /= rep_penalty\n",
    "\n",
    "        # B. Apply Temperature\n",
    "        logits = logits / temp\n",
    "\n",
    "        # C. Ban undesirable tokens (EOS and early newlines)\n",
    "        logits[0, eos_token] = -float(\"inf\")\n",
    "        for nl in newline_tokens:\n",
    "            logits[0, nl] = -float(\"inf\")\n",
    "\n",
    "        # D. Nucleus (Top-P) Sampling\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "\n",
    "        # Create mask for tokens to remove\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0 # Always keep at least one token\n",
    "\n",
    "        # Scatter the mask back to original indices\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = -float(\"inf\")\n",
    "\n",
    "        # E. Final Sample\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        next_token = torch.multinomial(probs, num_samples=1)\n",
    "        \n",
    "        # Update trackers\n",
    "        current_ids = torch.cat([current_ids, next_token], dim=-1) # Just for full context record\n",
    "        lntxt_ids.append(next_token.item())\n",
    "        \n",
    "        # F. Smart Stopping Logic\n",
    "        # Decode only the current line to check length/punctuation\n",
    "        line_text = tokenizer.decode(lntxt_ids)\n",
    "        words = line_text.strip().split()\n",
    "        word_count = len(words)\n",
    "\n",
    "        # Check for punctuation end\n",
    "        is_end_punct = line_text.strip() and line_text.strip()[-1] in \".?!;\"\n",
    "\n",
    "        if word_count >= wplnmax:\n",
    "            comp = True\n",
    "        elif word_count >= wpln and is_end_punct:\n",
    "            comp = True\n",
    "\n",
    "    # Formatting and storage\n",
    "    final_line = tokenizer.decode(lntxt_ids).strip()\n",
    "    lns.append(final_line)\n",
    "    \n",
    "    # Update input_ids for the next line's repetition penalty context\n",
    "    input_ids = torch.cat([input_ids, torch.tensor([lntxt_ids]).to(device)], dim=-1)\n",
    "    \n",
    "    print(f\"Line {i+1}: {final_line}\")\n",
    "\n",
    "print(\"\\n--- Final Composition ---\")\n",
    "print(txt)\n",
    "for line in lns:\n",
    "    print(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
