{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2074aae8-4ff4-468f-bf6e-f8dadb2cb2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aa/Desktop/coding/p3ai/t2final/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AI model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 76/76 [00:00<00:00, 1344.78it/s, Materializing param=transformer.wte.weight]            \n",
      "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# Load model\n",
    "print(\"Loading AI model...\")\n",
    "model_name = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e93accd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start prompt: 'In the quiet gallery of ancient echoes, where the marble statues breathe the dust of centuries past,'\n",
      "Generating...\n",
      "\n",
      "Generating Line 1...\n",
      "Line 1: the marble murals and paintings of the past are no longer a\n",
      "Generating Line 2...\n",
      "Line 2: scene of history. The world is no longer an art museum, as\n",
      "Generating Line 3...\n",
      "Line 3: a museum is a small, yet distinct place.\n",
      "Generating Line 4...\n",
      "Line 4: It is a place of art that is not, and is no\n",
      "\n",
      "Full Poem:\n",
      "1. the marble murals and paintings of the past are no longer a\n",
      "2. scene of history. The world is no longer an art museum, as\n",
      "3. a museum is a small, yet distinct place.\n",
      "4. It is a place of art that is not, and is no\n"
     ]
    }
   ],
   "source": [
    "txt = \"In the quiet gallery of ancient echoes, where the marble statues breathe the dust of centuries past,\" ##initial prompt\n",
    "print(f\"Start prompt: '{txt}'\");\n",
    "print(\"Generating...\\n\");\n",
    "\n",
    "ctxt = txt; ##the current text\n",
    "lns = []; ##store the lines as we make them in an arr to format later\n",
    "nlns = 4; ##number of lines in the poem\n",
    "wpln = 4; ##min amt of words per line\n",
    "wplnmax = 12; ##max amt of words per line\n",
    "newline_ids = tokenizer.encode(\"\\n\", add_special_tokens=False); ##newline token ids to mask during line generation\n",
    "\n",
    "for i in range(nlns): ##for each line\n",
    "    print(f\"Generating Line {i+1}...\");\n",
    "    lntxt = \"\"; ##make a string to store the current line\n",
    "    comp = False; ##flag to store line completion\n",
    "    while not comp: ##keep generating until the line is done\n",
    "        ##encode the text to get input as ids\n",
    "        inids = tokenizer.encode(ctxt, return_tensors=\"pt\"); ##inids = input_ids;\n",
    "\n",
    "        ##get predictions\n",
    "        with torch.no_grad(): ##dont do gradients\n",
    "            out = model(inids); ##eval model to get output\n",
    "            pred = out.logits; ##take the logits of the output as the predictions\n",
    "\n",
    "        ##get the next tokens probabilities to then sample from\n",
    "        nxttknlogits = pred[0,-1,:]; ##get the last tokens logits\n",
    "        for nid in newline_ids: \n",
    "            nxttknlogits[nid] = -float(\"inf\");\n",
    "        nxttknlogits[tokenizer.eos_token_id] = -float(\"inf\"); ##get rid of |endoftext| token to ensure we dont wander and keep same creativity\n",
    "        ##this sometimes produces bad results as the model cannot stop and start a new thought, and will sometimes just keep repeating itself\n",
    "            \n",
    "        temp=0.82; ##higher is more random, vice versa (from testing 0.78-0.84 is the best range)\n",
    "        nxttknlogits = nxttknlogits/temp; ##apply the temperature to logits to have some control over randomness;\n",
    "        nxttknprobs = torch.softmax(nxttknlogits, dim=0); ##get the probabilities through softmax (we use softmax bc logits can be negative and softmax nomralizes them to be between 0 and 1);\n",
    "\n",
    "        p = 0.86; ##controls the nucleus sampling threshold (from testing 0.84 to 0.89 is the best range)\n",
    "        \n",
    "        sortprobs, sortidxs = torch.sort(nxttknprobs, descending=True); ##sort the probabilities and their associated indexes\n",
    "        cumulativeprobs = torch.cumsum(sortprobs, dim=0); ##we need to get the cumulative probabilities to find the smallest set that passes the threshold (nucleus i think)\n",
    "        \n",
    "        sortidxtorm = cumulativeprobs>p; ##get a mask fo the indexes to rm based on the threshold;\n",
    "        sortidxtorm[1:] = sortidxtorm[:-1].clone(); ##shift the mask to the right by 1 so that we keep the first token that passes the threshold;\n",
    "        sortidxtorm[0] = False; ##make sure the first idx is never rmed; ()\n",
    "        \n",
    "        sortprobs[sortidxtorm] = 0.0; ##remove the probabilities that are above the threshold;\n",
    "        sortprobs = sortprobs/sortprobs.sum(); ##normalize the rem probabilities to 1\n",
    "        \n",
    "        idxinsort = torch.multinomial(sortprobs, 1).item(); ##take a choice based on the remaining probs\n",
    "        nxttknid = sortidxs[idxinsort].item(); ##get the tkn id for this idx\n",
    "\n",
    "        \"\"\" The above section replaces the top k samples as it is better for thresholding and overall creativity of the model; It also lowers the odds of getting a random eof or eol token.\n",
    "        ############### TOP K (5) SAMPLING ###############\n",
    "        # the top k samples thing\n",
    "        k=5; ##top 5 choices will be considered then randomly chosen after normalizing\n",
    "        topprobs, topidxs = torch.topk(nxttknprobs, k);\n",
    "        \n",
    "        topprobs = topprobs/topprobs.sum(); ##normalize the probabilities\n",
    "        idx = torch.multinomial(topprobs, 1).item(); ##randomly take a choice based on the probs\n",
    "        nxttknid = topidxs[idx].item(); ##get the token id associated with this index\n",
    "        \"\"\"\n",
    "\n",
    "        nxtword = tokenizer.decode([nxttknid]); ##decode to get the string representation of the word\n",
    "        lntxt+=nxtword; ##add to the line text\n",
    "        ctxt+=nxtword; ##add to the current text for next iter\n",
    "        \n",
    "        wcnt = len(lntxt.strip().split()); ##count words on the line\n",
    "        if (wcnt>=wpln and lntxt.strip().endswith((\".\", \"!\", \"?\"))): comp = True;\n",
    "        elif (wcnt>=wplnmax): comp = True;\n",
    "\n",
    "    lns.append(lntxt.strip()); ##after line is done store it\n",
    "    print(f\"Line {i+1}: {lntxt.strip()}\"); ##then output it as an individual line\n",
    "    ##ctxt+=\"\\n\"; ##add a new line so next line will start on a new line i guess?\n",
    "\n",
    "print(\"\\nFull Poem:\");\n",
    "for i, ln in enumerate(lns, 1): print(f\"{i}. {ln}\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49faa5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
